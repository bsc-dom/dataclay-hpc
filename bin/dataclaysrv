#!/bin/bash
set -e
PREFIX="[dataclaysrv]"
function dataclayecho(){ echo "$PREFIX ${1}"; }
function dataclayerr(){ echo "!! $PREFIX ERROR: ${1}"; exit 1; }
function dataclaywarn(){ echo "$PREFIX WARNING: ${1}"; }
function dataclayinfo(){ echo "$PREFIX ${1}"; }

#=== FUNCTION ================================================================
# NAME: usage
# DESCRIPTION: Display usage information for this script.
# PARAMETER 1: ---
#=============================================================================
function create_globaljob_config {
	mkdir -p $DATACLAY_JOB_FOLDER
	# Global job config
	echo "######## Global environment variables ########" > $GLOBAL_JOB_CONFIG
	echo "export DATACLAY_VERSION=$DATACLAY_VERSION" >> $GLOBAL_JOB_CONFIG
	echo "export DATACLAY_JOBID=$DATACLAY_JOBID" >> $GLOBAL_JOB_CONFIG
	echo "export LMNODE=$LMNODE" >> $GLOBAL_JOB_CONFIG
	echo "export CLIENTNODE=$CLIENTNODE" >> $GLOBAL_JOB_CONFIG
	echo "export DSNODES=\"$DSNODES\"" >> $GLOBAL_JOB_CONFIG
	echo "export PYTHON_EE_PER_NODE=$PYTHON_EE_PER_NODE" >> $GLOBAL_JOB_CONFIG
	echo "export JAVA_EE_PER_NODE=$JAVA_EE_PER_NODE" >> $GLOBAL_JOB_CONFIG	
	echo "export PROLOG=$PROLOG" >> $GLOBAL_JOB_CONFIG
	echo "export DEBUG=$DEBUG" >> $GLOBAL_JOB_CONFIG
	echo "export DATACLAY_PROLOGS=$DATACLAY_PROLOGS" >> $GLOBAL_JOB_CONFIG	
	echo "export DATACLAY_JAR=$DATACLAY_JAR" >> $GLOBAL_JOB_CONFIG
	echo "export DATACLAY_USR=$DATACLAY_USR" >> $GLOBAL_JOB_CONFIG
	echo "export DATACLAY_PWD=$DATACLAY_PWD" >> $GLOBAL_JOB_CONFIG
	echo "export DATACLAY_DATASET=$DATACLAY_DATASET" >> $GLOBAL_JOB_CONFIG
	echo "export PYTHONPATH=$DATACLAY_PYTHONPATH:\$PYTHONPATH" >> $GLOBAL_JOB_CONFIG
	echo "export LD_LIBRARY_PATH=$USR_LIB:\$LD_LIBRARY_PATH" >> $GLOBAL_JOB_CONFIG
	echo "export PATH=$DATACLAY_PATH:\$PATH" >> $GLOBAL_JOB_CONFIG
}

function set_host_envs {
	JOB_CONFIG=$DATACLAY_JOB_FOLDER/${DATACLAY_HOST}.config
	# Do not use DATACLAY_JOB_FOLDER since it is expanding current $HOME and not remote one
	JOB_FOLDER="\$HOME/.dataClay/$DATACLAY_JOBID/${DATACLAY_HOST}"
	
	if [ $LMNODE == "localhost" ] || [ $LMNODE == "127.0.0.1" ]; then 
		LOGICMODULE_IP=127.0.0.1
	else 
		LOGICMODULE_IP=`host ${LMNODE} | rev | cut -d ' ' -f1 | rev`
	fi
	#CLIENT_IP=`host ${CLIENTNODE} | rev | cut -d ' ' -f1 | rev`
	STORAGE_PATH="$JOB_FOLDER/storage"
	APP_PATH="$JOB_FOLDER/app"
	APP_BIN_PATH="$APP_PATH/bin"
	MODEL_PATH="$JOB_FOLDER/model"
	MODEL_BIN_PATH="$MODEL_PATH/bin"
	STUBS_PATH="$APP_PATH/stubs"
	DEPLOY_PATH="$STORAGE_PATH/deploy_path/"
	DEPLOY_PATH_SRC="$DEPLOY_PATH/src"	
	DATACLAYCLIENTCONFIG="$JOB_FOLDER/cfgfiles/client.properties"
	DATACLAYGLOBALCONFIG="$JOB_FOLDER/cfgfiles/global.properties"
	DATACLAYSESSIONCONFIG="$JOB_FOLDER/cfgfiles/session.properties"
	JOB_ENV_FILE="$JOB_FOLDER/env.sh"
}

function create_job_config {
		
	# Host job config
	cat $GLOBAL_JOB_CONFIG > $JOB_CONFIG
	echo "######## $DATACLAY_HOST environment variables ########" >> $JOB_CONFIG
	echo "export TRACING=$TRACING" >> $JOB_CONFIG
	echo "export JOB_FOLDER=$JOB_FOLDER" >> $JOB_CONFIG
	echo "export JOB_ENV_FILE=$JOB_ENV_FILE" >> $JOB_CONFIG	
	
	echo "export FLAGS=\"$FLAGS\"" >> $JOB_CONFIG
	echo "export SHUTDOWN_TIMEOUT=$SHUTDOWN_TIMEOUT" >> $JOB_CONFIG
	
	echo "export DATACLAYGLOBALCONFIG=$DATACLAYGLOBALCONFIG" >> $JOB_CONFIG
	echo "export DATACLAYCLIENTCONFIG=$DATACLAYCLIENTCONFIG" >> $JOB_CONFIG
	echo "export DATACLAYSESSIONCONFIG=$DATACLAYSESSIONCONFIG" >> $JOB_CONFIG
	echo "export APP_PATH=$APP_PATH" >> $JOB_CONFIG
	echo "export APP_BIN_PATH=$APP_BIN_PATH" >> $JOB_CONFIG
	
	echo "export MODEL_PATH=$MODEL_PATH" >> $JOB_CONFIG
	echo "export MODEL_BIN_PATH=$MODEL_BIN_PATH" >> $JOB_CONFIG
	
	echo "export STUBS_PATH=$STUBS_PATH" >> $JOB_CONFIG
	echo "################" >> $JOB_CONFIG
	
}

function create_script { 
	SCRIPT_PATH=$1
	echo "#!/bin/bash" > $SCRIPT_PATH	
	echo "set -e" >> $SCRIPT_PATH
	cat $JOB_CONFIG >> $SCRIPT_PATH
	if [ ! -z $PROLOG ]; then 
		cat $PROLOG >> $SCRIPT_PATH
		printf "\n" >> $SCRIPT_PATH 
	fi
	
	
}

function generate_env_file {
	# NOT ALL VARIABLES ARE USED (i.e. logicmodule port just for logicmodule image...)

	SERVICEIDX=$1
	# Generic vars
	ENV_FILE=$JOB_FOLDER/env_"$SERVICEIDX".sh
	echo "echo \"export PATH=\$PATH:\\\$PATH\" > $ENV_FILE" >> $START_SCRIPT
    echo "echo \"export PYTHONPATH=\$PYTHONPATH:\\\$PYTHONPATH\" >> $ENV_FILE" >> $START_SCRIPT
    echo "echo \"export LD_LIBRARY_PATH=\$LD_LIBRARY_PATH:\\\$LD_LIBRARY_PATH\" >> $ENV_FILE" >> $START_SCRIPT
    echo "echo \"export LOGICMODULE_PORT_TCP=1111\" >> $ENV_FILE " >> $START_SCRIPT
	echo "echo \"export DATACLAY_ADMIN_USER=admin\" >> $ENV_FILE" >> $START_SCRIPT
	echo "echo \"export DATACLAY_ADMIN_PASSWORD=admin\" >> $ENV_FILE" >> $START_SCRIPT
	echo "echo \"export DATACLAYGLOBALCONFIG=$DATACLAYGLOBALCONFIG\" >> $ENV_FILE" >> $START_SCRIPT
	
	# DSs
	echo "echo \"export DEPLOY_PATH=$DEPLOY_PATH\" >> $ENV_FILE" >> $START_SCRIPT
	echo "echo \"export DEPLOY_PATH_SRC=$DEPLOY_PATH_SRC\" >> $ENV_FILE" >> $START_SCRIPT
	echo "echo \"export LOGICMODULE_HOST=$LOGICMODULE_IP\" >> $ENV_FILE" >> $START_SCRIPT
	echo "echo \"export DATASERVICE_NAME=$DATACLAY_HOST\" >> $ENV_FILE" >> $START_SCRIPT
	echo "echo \"export DATASERVICE_JAVA_PORT_TCP=$DS_PORT\" >> $ENV_FILE" >> $START_SCRIPT
	echo "echo \"export DATASERVICE_PYTHON_PORT_TCP=$DS_PORT\" >> $ENV_FILE" >> $START_SCRIPT
	
	DS_PORT=`expr $DS_PORT + 1`
}

function generate_global_properties { 
    echo "echo \"STORAGE_PATH=$STORAGE_PATH\" > $DATACLAYGLOBALCONFIG" >> $START_SCRIPT
    echo "echo \"STATE_FILE_PATH=$STORAGE_PATH/state.txt\" >> $DATACLAYGLOBALCONFIG" >> $START_SCRIPT
    echo "echo \"EE_PERSISTENT_INFO_PATH=$STORAGE_PATH/\" >> $DATACLAYGLOBALCONFIG" >> $START_SCRIPT
    echo "echo \"DEFAULT_GLOBALGC_CACHE_PATH=$STORAGE_PATH/\" >> $DATACLAYGLOBALCONFIG" >> $START_SCRIPT
	if [ ! -z "$GLOBAL_PROPS" ] ; then
		while IFS= read -r line
		do
			dataclaywarn "Found global properties configuration: $line"
    		echo "echo \"$line\" >> $DATACLAYGLOBALCONFIG" >> $START_SCRIPT
		done < "$GLOBAL_PROPS"
	fi
	if [ "$DEBUG" == True ]; then 
   		echo "echo \"CHECK_LOG4J_DEBUG=true\" >> $DATACLAYGLOBALCONFIG" >> $START_SCRIPT
	fi 
}

function generate_client_properties { 
    echo "echo \"HOST=$LOGICMODULE_IP\" > $DATACLAYCLIENTCONFIG" >> $START_SCRIPT
    echo "echo \"TCPPORT=1111\" >> $DATACLAYCLIENTCONFIG" >> $START_SCRIPT
}

function generate_session_properties { 

    echo "echo \"DataClayClientConfig=$DATACLAYCLIENTCONFIG\" > $DATACLAYSESSIONCONFIG" >> $START_SCRIPT
    echo "echo \"Account=$DATACLAY_USR\" >> $DATACLAYSESSIONCONFIG" >> $START_SCRIPT
    echo "echo \"Password=$DATACLAY_PWD\" >> $DATACLAYSESSIONCONFIG" >> $START_SCRIPT
    echo "echo \"StubsClasspath=$STUBS_PATH\" >> $DATACLAYSESSIONCONFIG" >> $START_SCRIPT
    echo "echo \"DataSetForStore=$DATACLAY_DATASET\" >> $DATACLAYSESSIONCONFIG" >> $START_SCRIPT
    echo "echo \"DataSets=$DATACLAY_DATASET\" >> $DATACLAYSESSIONCONFIG" >> $START_SCRIPT
    #echo "echo \"DataClayGlobalConfig=$DATACLAYGLOBALCONFIG\" >> $DATACLAYSESSIONCONFIG" >> $START_SCRIPT
	if [ "$TRACING" = true ] ; then
    	echo "echo \"Tracing=True\" >> $DATACLAYSESSIONCONFIG" >> $START_SCRIPT
		#IFS=', ' read -r -a HOSTS_ARRAY <<< "$HOSTS"
    	#echo "echo \"ExtraeStartingTaskID=${#HOSTS_ARRAY[@]}\" >> $DATACLAYSESSIONCONFIG" >> $START_SCRIPT
	fi
}

function link_singularity {
	#### create link to singularity images for using it in frontend (or for backends without internet access and shared FS)
	IMAGE=$1
	SINGULARITY_JOB_FOLDER=$DATACLAY_JOB_FOLDER/$DATACLAY_HOST/images
	mkdir -p $SINGULARITY_JOB_FOLDER
	touch $SINGULARITY_JOB_FOLDER/Singularity
	ln -s $SINGULARITY_IMAGES_HOME/$IMAGE  $SINGULARITY_JOB_FOLDER/$IMAGE
}

# ===================== DEPLOYMENT SCRIPTS ======================================= #

function deploy { 
	HOSTNAME=$1
	DATACLAY_HOST=$2 #node name (logicmodule, DS1, DS2, client,... )
	SERVICES=(${3}) #services (logicmodule, dsjava, dspython, client) 
	INSTANCES_PER_SERVICE=(${4}) #number of instances per service
	
	dataclayecho "Deploying $DATACLAY_HOST to $HOSTNAME..."
	set_host_envs
	create_job_config
		
	DEPLOY_SCRIPT=$(mktemp /tmp/${DATACLAY_JOBID}_${DATACLAY_HOST}_deploy.XXXX)
	create_script $DEPLOY_SCRIPT 	
	
	# link to singularity images in frontend (for shared fs)
	for SERVICE in ${SERVICES[@]}; do
		link_singularity ${SERVICE}.sif
	done
	
	
	####### DEPLOY SCRIPT ####### 
	# prepare paths
	echo "mkdir -p $JOB_FOLDER" >> $DEPLOY_SCRIPT
	echo "mkdir -p $JOB_FOLDER/cfgfiles" >> $DEPLOY_SCRIPT
	echo "mkdir -p $DEPLOY_PATH" >> $DEPLOY_SCRIPT
	echo "mkdir -p $DEPLOY_PATH_SRC" >> $DEPLOY_SCRIPT
	echo "mkdir -p $STORAGE_PATH" >> $DEPLOY_SCRIPT
	echo "rm -rf \$HOME/.singularity/instances/logs" >> $DEPLOY_SCRIPT
		# First generate singularity-compose.yml
	SINGULARITY_COMPOSE_FILE=/tmp/${DATACLAY_HOST}-singularity-compose.yml
	
	cat $SC_TEMPLATES_FOLDER/header.yml > $SINGULARITY_COMPOSE_FILE
	
	sed -i "s/dataclayversion/$DATACLAY_VERSION/g" $SINGULARITY_COMPOSE_FILE
	i=0
	for SERVICE in ${SERVICES[@]}; do
		NUM_INSTANCES=${INSTANCES_PER_SERVICE[$i]}
		for j in $(seq 1 $NUM_INSTANCES); do
			if [ ${#SERVICES[@]} == 1 ]; then SERVICENAME=${DATACLAY_HOST}_${j}
			else SERVICENAME=${DATACLAY_HOST}_${SERVICE}_${j}; fi 
			
			cat $SC_TEMPLATES_FOLDER/instance.yml >> $SINGULARITY_COMPOSE_FILE
			sed -i "s/dataclayservice/${SERVICENAME}/g" $SINGULARITY_COMPOSE_FILE
			sed -i "s/\.\/env\.sh/.\/env_${SERVICENAME}.sh/g" $SINGULARITY_COMPOSE_FILE
			
			echo "ln -sf $JOB_FOLDER/images/${SERVICE}.sif $JOB_FOLDER/images/${SERVICENAME}.sif" >> $DEPLOY_SCRIPT
		done
		i=$(($i + 1))
	done
	
	sed -i "s/USR_LIB/$SINGULARITY_USR_LIB/g" $SINGULARITY_COMPOSE_FILE
	sed -i "s/command: \"\"/command: \"$FLAGS\"/g" $SINGULARITY_COMPOSE_FILE
	
	# Now cat it to deployment script 
	echo "echo \"" >> $DEPLOY_SCRIPT
	cat $SINGULARITY_COMPOSE_FILE >> $DEPLOY_SCRIPT				
	echo "\" > $JOB_FOLDER/singularity-compose.yml" >> $DEPLOY_SCRIPT
	rm $SINGULARITY_COMPOSE_FILE
	
	echo "rm -rf \$HOME/.singularity/instances/logs/$HOSTNAME" >> $DEPLOY_SCRIPT
	
	
	for SERVICE in ${SERVICES}; do
		echo "if [ ! -f $JOB_FOLDER/images/Singularity ]; then touch $JOB_FOLDER/images/Singularity; fi" >> $DEPLOY_SCRIPT
		echo "if [ ! -f $JOB_FOLDER/images/${SERVICE}.sif ]; then
			singularity pull $JOB_FOLDER/images/${SERVICE}.sif library://support-dataclay/default/${SERVICE}:$DATACLAY_VERSION;
		fi" >> $DEPLOY_SCRIPT
	done
	
	# Deploy 
	ssh "${HOSTNAME}" "bash -s" < $DEPLOY_SCRIPT
	
	# Send log4j configuration
	scp -q $LOG4J_CONFIG $HOSTNAME:$JOB_FOLDER/cfgfiles/log4j2.xml
	# Send script
	chmod +x $DEPLOY_SCRIPT
	scp -q $DEPLOY_SCRIPT $HOSTNAME:$JOB_FOLDER/deploy
	
	# Clean temporary scripts
	rm $DEPLOY_SCRIPT
	
	dataclayecho "${DATACLAY_HOST} deployed to $HOSTNAME"
}


# ===================== START SCRIPTS ======================================= #

function start { 
	HOSTNAME=$1
	DATACLAY_HOST=$2 #node name (LM, DS1, DS2, CL,... ) #client host must be called CL
	SERVICES=(${3}) #services (logicmodule, dsjava, dspython, client) 
	INSTANCES_PER_SERVICE=(${4}) #number of instances per service
	
	set_host_envs
	create_job_config

	START_SCRIPT=$(mktemp /tmp/${DATACLAY_JOBID}_${DATACLAY_HOST}_start.XXXX) # i.e. 24000_LM
	create_script $START_SCRIPT 
	
	# generate global properties 
	generate_global_properties
	
	i=0
	for SERVICE in ${SERVICES[@]}; do
		NUM_INSTANCES=${INSTANCES_PER_SERVICE[$i]}
		for j in $(seq 1 $NUM_INSTANCES); do
			if [ ${#SERVICES[@]} == 1 ]; then SERVICENAME=${DATACLAY_HOST}_${j}
			else SERVICENAME=${DATACLAY_HOST}_${SERVICE}_${j}; fi 
			generate_env_file $SERVICENAME
		done
		i=$(($i + 1))
	done 
	
	
	#### TODO: specialize this
	if [ $SERVICE != "client" ]; then 
		echo "cd $JOB_FOLDER; singularity-compose up" >> $START_SCRIPT
	else 
		if [ ! -f $DATACLAY_JOB_FOLDER/client.config ]; then 
			# in case client is named differently
			ln -s $JOB_CONFIG $DATACLAY_JOB_FOLDER/client.config
		fi
		echo "mkdir -p $APP_PATH" >> $START_SCRIPT
		echo "mkdir -p $APP_BIN_PATH" >> $START_SCRIPT
		echo "mkdir -p $STUBS_PATH" >> $START_SCRIPT
		echo "mkdir -p $MODEL_PATH" >> $START_SCRIPT
		echo "mkdir -p $MODEL_BIN_PATH" >> $START_SCRIPT
		# Generate client properties 
		generate_client_properties
		# Generate session.properties. If --tracing was provided, add session.properties field. 
		generate_session_properties
	fi
	
	# Send scripts
	echo "mkdir -p $JOB_FOLDER" | ssh "${HOSTNAME}" bash -s #sanity check
	chmod +x $START_SCRIPT
	scp -q $START_SCRIPT $HOSTNAME:$JOB_FOLDER/start
	
	# Start 
	echo "$JOB_FOLDER/start" | ssh "${HOSTNAME}" bash -s
	
	# Clean temporary scripts
	rm $START_SCRIPT

}


function deploy_client {
	#### SPECIFIC DEPLOYMENT FOR CLIENT NODES #### 
	HOSTNAME=$1
	DATACLAY_HOST=$2 #node name (LM, DS1, DS2, client,... )
	
	set_host_envs
	create_job_config
	
	CLIENT_SCRIPT=$(mktemp /tmp/${DATACLAY_JOBID}_${DATACLAY_HOST}_client.XXXX)	
	create_script $CLIENT_SCRIPT 		
	create_job_config
	
	####### CLIENT SCRIPT ####### 
	echo "cd $APP_PATH"  >> $CLIENT_SCRIPT
	echo "singularity \$1 -B $JOB_FOLDER/cfgfiles/log4j2.xml:/home/dataclayusr/dataclay/logging/log4j2.xml \
		-B $USR_LIB:/usr/lib64/ -B $JOB_FOLDER/env_client_1.sh:/.singularity.d/env/python_env.sh \
		$JOB_FOLDER/images/client.sif \${@:2}" >> $CLIENT_SCRIPT
	
	# Send scripts
	echo "mkdir -p $JOB_FOLDER" | ssh "${HOSTNAME}" bash -s #sanity check
	chmod +x $CLIENT_SCRIPT
	scp -q $CLIENT_SCRIPT $HOSTNAME:$JOB_FOLDER/client
		
	rm $CLIENT_SCRIPT
}

# ===================== STOP SCRIPTS ======================================= #
function stop { 
	HOSTNAME=$1
	DATACLAY_HOST=$2 #node name (LM, DS1, DS2, CL,... )
	SERVICES=(${3}) #services (logicmodule, dsjava, dspython, client) 
	INSTANCES_PER_SERVICE=(${4}) #number of instances per service
	
	set_host_envs
	create_job_config
	
	dataclayecho "Stopping ${DATACLAY_HOST} at $HOSTNAME"
	STOP_SCRIPT=$(mktemp /tmp/${DATACLAY_JOBID}_${DATACLAY_HOST}_stop.XXXX)
	create_script $STOP_SCRIPT
	
	i=0
	for SERVICE in ${SERVICES[@]}; do
		NUM_INSTANCES=${INSTANCES_PER_SERVICE[$i]}
		for j in $(seq 1 $NUM_INSTANCES); do
			if [ ${#SERVICES[@]} == 1 ]; then SERVICENAME=${DATACLAY_HOST}_${j}
			else SERVICENAME=${DATACLAY_HOST}_${SERVICE}_${j}; fi 
			SIGNAL="SIGTERM"
			if [ $SERVICE == "dspython" ]; then 
				SIGNAL="SIGINT"
			fi 
			echo "if singularity instance list | grep $SERVICENAME; then" >>  $STOP_SCRIPT
			echo "singularity instance stop -s $SIGNAL -t $SHUTDOWN_TIMEOUT $SERVICENAME" >> $STOP_SCRIPT
			echo "fi" >> $STOP_SCRIPT
		done
		i=$(($i + 1))
	done
	

	# Send scripts
	echo "mkdir -p $JOB_FOLDER" | ssh "${HOSTNAME}" bash -s #sanity check
	chmod +x $STOP_SCRIPT
	scp -q $STOP_SCRIPT $HOSTNAME:$JOB_FOLDER/stop
	
	# Deploy 
	echo "$JOB_FOLDER/stop" | ssh "${HOSTNAME}" bash -s
	
	# Clean temporary scripts
	rm $STOP_SCRIPT
}

function clean { 
	HOSTNAME=$1
	DATACLAY_HOST=$2 #node name (LM, DS1, DS2, CL,... )
	SERVICES=(${3}) #services (logicmodule, dsjava, dspython, client) 
	INSTANCES_PER_SERVICE=(${4}) #number of instances per service
	
	set_host_envs
	echo "rm -rf $JOB_FOLDER" | ssh "${HOSTNAME}" bash -s
}

# ==================== SERVICES ============================ #

function dataclaydeploy { 
	
	dataclayclean

  	dataclaywarn "Job was not deployed. Going to deploy."
	dataclayinfo "========== Deploying dataClay =========="

	dataclayecho "- deploying to hosts = \"$HOSTS\""
	HOSTS=($(echo $HOSTS | tr " " "\n"))
	if [ "${#HOSTS[@]}" -lt 3 ]; then
		dataclayerr "Minimum 3 hosts must be provided (logic module, a data service node and a client node)"
	fi
	CLIENTNODE=${HOSTS[0]} #1st node for client 
	LMNODE=${HOSTS[1]}     #2nd node for LM
	DSNODES=${HOSTS[@]:2}  # DS nodes

	# Check if Singularity images exist 
	if [ ! -d "$SINGULARITY_IMAGES_HOME" ]; then 
		dataclaywarn "Singularity images not found at $SINGULARITY_IMAGES_HOME. Make sure installation was correct."
		exit 1
	fi
	create_globaljob_config

	deploy $LMNODE ${LM_HOSTID} "logicmodule"
	i=1
	for NODE in $DSNODES; do
		deploy $NODE ${DSNAME_PREFIX}${i} "dsjava dspython" "$JAVA_EE_PER_NODE $PYTHON_EE_PER_NODE"
		i=$(($i + 1))
	done
	deploy $CLIENTNODE ${CLIENT_HOSTID} "client"
	deploy_client $CLIENTNODE ${CLIENT_HOSTID}
	
	echo "$DATACLAY_JOBID" >> $DEPLOYED_DATACLAY_JOBS
	dataclayinfo "========== dataClay deployed! =========="
}

function dataclaystart { 
	dataclayinfo "========== Starting dataClay ========== "

	# Get client config to get DS nodes and LM node
	source $GLOBAL_JOB_CONFIG
	start $LMNODE ${LM_HOSTID} "logicmodule"
	i=1
	for NODE in $DSNODES; do
		start $NODE ${DSNAME_PREFIX}${i} "dsjava dspython" "$JAVA_EE_PER_NODE $PYTHON_EE_PER_NODE"
		i=$(($i + 1))
	done
	start $CLIENTNODE ${CLIENT_HOSTID} "client"
	
	############ VERIFY ############ 
	# Wait for dataClay to be read
	$SCRIPTDIR/dataclay WaitForDataClayToBeAlive 20 3
	
	# Wait for backends 
	for DSNODE in $DSNODES; do
	   DSCOUNTER=0
	   dataclayecho "Waiting for $DSNODE Java execution environments to be ready... "
	   while [ $DSCOUNTER -ne $JAVA_EE_PER_NODE ]; do
	       DSCOUNTER=`$SCRIPTDIR/dataclay GetBackends admin admin java | grep "${DSNAME_PREFIX}" | wc -l`
	   done
	   dataclayecho "$DSNODE Java execution environments are ready"
	
	   DSCOUNTER=0
	   dataclayecho "Waiting for $DSNODE Python execution environments to be ready... "
	   while [ $DSCOUNTER -ne $PYTHON_EE_PER_NODE ]; do
	       DSCOUNTER=`$SCRIPTDIR/dataclay GetBackends admin admin python | grep "${DSNAME_PREFIX}" | wc -l`
	   done
	   dataclayecho "$DSNODE Python execution environments are ready"
	done
	
	dataclayinfo "========== dataClay started! ========== "
}

function dataclaystop { 
	dataclayinfo "========== Stopping dataClay ========== "

	# ------------------------------ dataClay Job configuration -------------------------------------------
	# Get client config to get DS nodes and LM node
	GLOBAL_JOB_CONFIG="$HOME/.dataClay/$DATACLAY_JOBID/job.config"
	source $GLOBAL_JOB_CONFIG
	#------------------------------------------------------------------------------------------------------
	i=1
	for NODE in $DSNODES; do
		stop $NODE ${DSNAME_PREFIX}${i} "dsjava dspython" "$JAVA_EE_PER_NODE $PYTHON_EE_PER_NODE"
		i=$(($i + 1))
	done
	stop $LMNODE ${LM_HOSTID} "logicmodule"
	
	dataclayinfo "========== dataClay stopped! ========== "
}


function dataclayclean { 
	dataclayinfo "========== Cleaning dataClay ========== "

	# remove from deployed.jobs
	sed -i "/${DATACLAY_JOBID}$/d" $DEPLOYED_DATACLAY_JOBS
	# replace from job pids 
	sed -i "/${DATACLAY_JOBID}$/d" $DATACLAY_JOB_PIDS
	echo "$PPID=$DATACLAY_JOBID" >> $DATACLAY_JOB_PIDS
		
	if [ -d $DATACLAY_JOB_FOLDER ]; then
		dataclaystop
		i=1
		for NODE in $DSNODES; do
			clean $NODE ${DSNAME_PREFIX}${i} "dsjava dspython" "$JAVA_EE_PER_NODE $PYTHON_EE_PER_NODE"
			i=$(($i + 1))
		done
		clean $LMNODE ${LM_HOSTID} "logicmodule"
		clean $CLIENTNODE ${CLIENT_HOSTID} "client"
		rm -rf $DATACLAY_JOB_FOLDER
	fi
		
	# stop all remaining instances in current node (for localhost) 
	INSTANCES=$(singularity instance list | awk '{if(NR>1)print $1}')
	for INSTANCE in $INSTANCES; do
		dataclaywarn "Found orphan instance $INSTANCE. Killing..."
		singularity instance stop -s SIGKILL $INSTANCE
	done 
	
	dataclayinfo "========== dataClay cleaned! ========== "
	
}

# ==================== MAIN ============================ #

if [ "$#" -lt 1 ]; then
	dataclayerr "Please provide argument to start or stop dataClay" 
fi
START=false
STOP=false
RESTART=false
if [ "$1" == "start" ]; then 
	START=true 
elif [ "$1" == "stop" ]; then 
	STOP=true
elif [ "$1" == "restart" ]; then 
	RESTART=true 
else
	dataclayerr "First argument must be start or stop"
fi 
# Check DATACLAY_HOME is set 
SCRIPTDIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null && pwd )"
DATACLAY_HOME=$SCRIPTDIR/..
#if [ -z $DATACLAY_HOME ]; then
#        dataclayerr "Please set DATACLAY_HOME in your host (via bashrc, ...)!"
#fi 
if [ ! -d $HOME/.dataClay ]; then
	dataclaywarn "Creating $HOME/.dataClay folder." 
	mkdir $HOME/.dataClay 
fi
DATACLAY_VERSION=$(cat $DATACLAY_HOME/VERSION.txt)
DATACLAY_JOB_PIDS=$HOME/.dataClay/job.pids
DEPLOYED_DATACLAY_JOBS=$HOME/.dataClay/deployed.jobs

### PREPARE ###
if [ ! -f $DATACLAY_JOB_PIDS ]; then touch $DATACLAY_JOB_PIDS; fi
if [ ! -f $DEPLOYED_DATACLAY_JOBS ]; then touch $DEPLOYED_DATACLAY_JOBS; fi
 
ID=$($SCRIPTDIR/dataclayid $PPID)

# if ID exists, set it 
if [ ! -z $ID ]; then
	export DATACLAY_JOBID=$ID
    dataclaywarn "Found current process $PPID using DATACLAY_JOBID"
fi

# Check DATACLAY_JOBID is set 
if [ -z $DATACLAY_JOBID ]; then
	if [[ $(ls -l $HOME/.dataClay | grep "^d") ]]; then 
		LAST_JOB=$(basename $(ls -td -- $HOME/.dataClay/*/ | head -n 1))
	fi
	if [ -z "$LAST_JOB" ]; then
		NUMERIC_VERSION=${DATACLAY_VERSION%.dev}
		export DATACLAY_JOBID=$((${NUMERIC_VERSION//.} * 1000))
	else 
		export DATACLAY_JOBID=$(($LAST_JOB + 1))
	fi
    dataclaywarn "DATACLAY_JOBID environment not set. Generating one = $DATACLAY_JOBID"
else 
    dataclayinfo "Found DATACLAY_JOBID = $DATACLAY_JOBID"
fi

if [ -z $ID ]; then 
	# add parent pid to job pids of dataClay 
	echo "$PPID=$DATACLAY_JOBID" >> $DATACLAY_JOB_PIDS
fi
if [ -z $DATACLAY_JOBID ]; then
        dataclayerr "CRITICAL: please set DATACLAY_JOBID!"
fi
DEPLOYED=false
while IFS= read -r line; do
  if [ "$line" == "$DATACLAY_JOBID" ]; then
  	 DEPLOYED=true
  fi
done < "$DEPLOYED_DATACLAY_JOBS"

##### process arguments #####
shift

# global vars
DATACLAY_JOB_FOLDER="$HOME/.dataClay/$DATACLAY_JOBID"
GLOBAL_JOB_CONFIG=$DATACLAY_JOB_FOLDER/job.config
DATACLAY_PROLOGS=$DATACLAY_HOME/prolog
CLIENT_HOSTID="client"
LM_HOSTID="logicmodule"
DSNAME_PREFIX="dataservice"
DATACLAY_USR=bsc_user
DATACLAY_PWD=bsc_user
DATACLAY_DATASET=bsc_dataset
PYTHON_VERSION=3.7
DATACLAY_JAR="/home/dataclayusr/dataclay/dataclay.jar"
DATACLAY_PYTHONPATH="/home/dataclayusr/dataclay/dataclay_venv/lib/python${PYTHON_VERSION}/site-packages/"
DATACLAY_PYTHONPATH="$DATACLAY_PYTHONPATH:/home/dataclayusr/dataclay/dataclay_venv/lib/python${PYTHON_VERSION}"
if [ -d "/usr/lib64" ]; then 
	USR_LIB="/usr/lib64/"
else 
	USR_LIB="/usr/lib/"
fi
SINGULARITY_USR_LIB="\/usr\/lib\/"
DATACLAY_PATH="${DATACLAY_HOME}/bin"
JAVA_EE_PER_NODE=1
PYTHON_EE_PER_NODE=1
DS_PORT=2222
SINGULARITY_IMAGES_HOME=$DATACLAY_HOME/singularity/images/
LOG4J_CONFIG=$DATACLAY_HOME/logging/info.xml
DEBUG=False
SC_TEMPLATES_FOLDER=$DATACLAY_HOME/singularity/singularity-compose-templates
HOSTS="localhost localhost localhost" 

# Start args
TRACING=false
FLAGS=""
GLOBAL_PROPS=""

# Stop args
SHUTDOWN_TIMEOUT=300

# Args
while test $# -gt 0
do
	case "$1" in
			--python_ee_per_node) 
		    	shift 
		    	PYTHON_EE_PER_NODE=$1
	            dataclayecho "- setting python execution environment per node = $PYTHON_EE_PER_NODE"
		    	;;
			--hosts)
		   		shift
		    	HOSTS=$1
	            ;;
	        --prolog) 
	        	shift
	        	PROLOG=$1
	            dataclayecho "- setting prolog = $PROLOG"
	        	;;
	        --globalprops) 
	        	shift 
	        	GLOBAL_PROPS=$1 
	            dataclayecho "- deploying global properties at $GLOBAL_PROPS"
	        	;;
	        --cleandeploy) 
	        	DEPLOYED=false
	            dataclaywarn "Clean deploy option provided."
	        	;;
	        --debug) 
	            FLAGS="$FLAGS $1"
	            LOG4J_CONFIG=$DATACLAY_HOME/logging/debug.xml
	            DEBUG=True
	            dataclayecho "- debug mode: enabled "
	            ;; 
	        --tracing)
		    	TRACING=true
	            FLAGS="$FLAGS $1" 
	            dataclayecho "- tracing mode: enabled "
	            ;;
			--shutdown-timeout)
		    	shift
		    	SHUTDOWN_TIMEOUT=$1
		    	dataclayecho "- shutdown timeou = $TIMEOUT"
	            ;;
			--*) 
				dataclayerr "Wrong option $1 provided" 
				;;
			*)  
				dataclayerr "Wrong argument $1 provided"
            ;;
    esac
    shift
done

# if prolog is relative, check if exists in prolog folder 
if [ ! -z $PROLOG ]; then 
	if [[ $PROLOG != /* ]]; then 
		PROLOG=$DATACLAY_PROLOGS/$PROLOG
	fi
fi

if [ ! -f $PROLOG ]; then 
	dataclayerr "Prolog $PROLOG not found. Remember to provide absolute path for custom prologs."
fi

if [ "$DEPLOYED" == false ]; then 
	dataclaydeploy
else 
	dataclayinfo "INFO: Already deployed dataClay found."
fi

if [ "$START" == true ]; then 
	dataclaystart
elif  [ "$STOP" == true ]; then 
	dataclaystop 
elif  [ "$RESTART" == true ]; then 
	dataclaystop
	dataclaystart
fi 